---
title: "Breast cancer"
author: "Mariana Pasqualini"
date: "10/15/2020"
output: html_document
---

```{r setup, include=FALSE}
source("analysis.r")

opts <- options(knitr.kable.NA = "")
```

## Explorando os dados: Correlações e separabilidade das classes {.tabset}

### Mean

```{r mean-pairs, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.dim=c(8, 8)}
pairs[[1]]
```

É possível ver rapidamente que algumas variáveis estão correlacionadas fortemente entre si. 

Algumas variáveis apresentam correlação positiva quase perfeita (muito próximo de 1), como

- **radius_mean** e **perimeter_mean** (0.998)

- **radius_mean** e **area_mean** (0.987) 

- **perimeter_area** e **area_mean** (0.987)

Outras variáveis também apresentam uma correlação forte (acima de 0.5), como 

- **concavity_mean** e **compactness_mean** (0.883)

- **concave.points_mean** e **concavity_mean** (0.921)

- **concave.points_mean** e **perimeter_mean** (0.851)

Quanto à separabilidade dos dados, observando os histogramas, parece que as variáveis **radius_mean**, **perimeter_mean**, **area_mean** e **concavity_mean** ajudam a melhor identificar as populações _benigno_ e _maligno_, mas ainda assim vemos sobreposições e nenhuma variável parece separar perfeitamente essas classes.

### Standard Error

```{r se-pairs, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.dim=c(8, 8)}
pairs[[2]]
```

As correlações do erro padrão dessas variáveis são, de maneira geral, um pouco mais baixas, mas ainda sim temos correlações fortes nas mesmas variáveis da média. 

Quanto à separabilidade das classes, as variáveis parecem não ajudar a identificar as duas populações.

### Worst 

```{r worst-pairs, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.dim=c(8, 8)}
pairs[[3]]
```

Os scatterplots dessas variáveis, que são as médias dos maiores valores, também são bem parecidas com as observadas para as médias.

Por meio das variáveis **concave.points_worst**, **radius_worst** e **perimeter_worst** parecem ser as que mais ajudam separar as classes do diagnóstico.

## {-}

***

Quando uma variável tem correlação perfeita, sabendo o valor da variável X, conseguimos prever o de Y. Com isso, podemos escrever X em função de Y (ou vice-versa). E observamos nas matrizes acima que algumas variáveis têm correlação bem alta.

Trabalhar com dados altamente correlacionados: 

 - Não acrescenta informação da variabilidade dos dados
 - Os modelos podem levar mais tempo que o necessário para serem ajustados
 
 Para ajudar nesse problema, vamos recorrer à técnicas de redução de dimensionalidade.
 
 
### PCA

Análise de componentes principais é uma técnica de fatorização de matriz em que é possível explicar (parte da) a variabilidade dos dados por meio de combinações lineares não correlacionadas dos dados originais. De maneira geral, a ideia de fatorar uma matriz é escrever uma matriz "complicada" no produto de duas matrizes "mais simples".

Vamos definir as componentes principais por meio de operações matriciais, para conhecer todo o processo. 

1. Estimando a matriz de correlação:

O primeiro passo é estimar a matriz de correlação dos nossos dados. Na verdade, nessa etapa usa-se a matriz de covariâncias, mas um problema que pode surgir é que algumas variância são maiores que outras, e em casos muito discrepantes isso distorce as componentes principais. Para evitar isso, o procedimento é: padronizar a matriz de covariâncias e estimar as componentes a partir dessa matriz padronizada. Mas isso é o mesmo que usar a matriz de correlação e por isso vou seguir por ela.

```{r cor-matrix}

upper <- cor(data_processed[,-1])
upper[upper.tri(cor_matrix)] <- NA 


knitr::kable(round(upper[1:5, 1:5], 3),  caption = "Matriz de correlação: 5 primeiras linhas/colunas")
```

2. Obtendo os autovalores: 

```{r eigenvalues, collapse=TRUE, fig.align='center'}
eig$values

plot(eig$values)
abline(h = 1)
```

Para estimar o número de componentes principais ideal, um critério prático é observar os autovalores maiores ou iguais a 1, pois dessa forma as combinações lineares explicam a variância de uma variável original (padronizada) do dataset. Observando o scree-plot acima, é **k = 6** é o número de componentes principais adequados para explicar a estrutura da variância dos dados.

3. Obtendo os autovetores e escrevendo a 1ª componente principal:


Observando os autovetores associados aos seis autovalores identificados (apenas as 6 primeiras linhas de 30)

```{r eigenvectors, echo=FALSE, warning=FALSE, message=FALSE}

head(as.data.frame(eig$vectors[,1:6]))


```


Olhando um pouco para a teoria, a j-ésima componente principal é dada por:

$$
Ŷ_j = ê_{j1}X_{1} + ê_{j2}X_{2} + ... + ê_{jp}X_{p}
$$

E podemos escrever, para os dados do dataset em questão, a primeira componente principal da seguinte forma:

$$
Ŷ_1 = -0.2189\text{radius_mean} -0.1037\text{texture_mean} - 0.227\text{perimeter_mean} + ... - 0.132\text{fractal_dimension_worst}
$$

E assim em diante...

Mas como visualizações são mais interessantes, vamos construir gráficos!

#### Visualizando as componentes principais