---
title: "Breast cancer"
author: "Mariana Pasqualini"
date: "10/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(knitr.kable.NA = "", fig.align='center', fig.dim=c(8, 8))
```


## Motivação do classificador de câncer de mama

O objetivo desse notebook é aplicar técnicas de classificação aos dados do dataset _Breast cancer Wisconsin_, disponível [aqui](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). O interesse nesse dataset vem do _outubro rosa_, mês de conscientização para informar, alertar e previnir o câncer de mama. De acordo com o Instituto Nacional de Câncer (INCA), em 2019 foram estimados 59700 novos casos da doença no Brasil.

Quanto antes, melhor!

Esse estudo é **apenas** para fins didáticos e de aprendizado, que não necessariamente reflete a realidade ou uma opinião médica. 

## Pacotes

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(janitor)
library(GGally)
library(caret)
library(printr)
library(corrplot)
library(factoextra)
library(pROC)

source("analysis.r")
```


## Explorando os dados: Correlações e separabilidade das classes {.tabset}

### Mean

```{r mean-pairs, echo=FALSE, warning=FALSE, message=FALSE}
pairs[[1]]
```

É possível ver rapidamente que algumas variáveis estão correlacionadas fortemente entre si. 

Algumas variáveis apresentam correlação positiva quase perfeita (muito próximo de 1), como

- `radius_mean` e `perimeter_mean` (0.998)

- `radius_mean` e `area_mean` (0.987) 

- `perimeter_area` e `area_mean` (0.987)

Outras variáveis também apresentam uma correlação forte (acima de 0.5), como 

- `concavity_mean` e `compactness_mean` (0.883)

- `concave.points_mean` e `concavity_mean` (0.921)

- `concave.points_mean` e `perimeter_mean` (0.851)

Quanto à separabilidade dos dados, observando os histogramas, parece que as variáveis `radius_mean`, `perimeter_mean`, `area_mean` e `concavity_mean` ajudam a melhor identificar as populações _benigno_ e _maligno_, mas ainda assim vemos sobreposições e nenhuma variável parece separar perfeitamente essas classes.

### Standard Error

```{r se-pairs, echo=FALSE, warning=FALSE, message=FALSE}
pairs[[2]]
```

As correlações do erro padrão dessas variáveis são, de maneira geral, um pouco mais baixas, mas ainda sim temos correlações fortes nas mesmas variáveis da média. 

Quanto à separabilidade das classes, as variáveis parecem não ajudar a identificar as duas populações.

### Worst 

```{r worst-pairs, echo=FALSE, warning=FALSE, message=FALSE}
pairs[[3]]
```

Os scatterplots dessas variáveis, que são as médias dos maiores valores, também são bem parecidas com as observadas para as médias.

Por meio das variáveis `concave.points_worst`, `radius_worst` e `perimeter_worst` parecem ser as que mais ajudam separar as classes do diagnóstico.

## {-}

Quando uma variável tem correlação perfeita, sabendo o valor da variável X, conseguimos prever o de Y. Com isso, podemos escrever X em função de Y (ou vice-versa). E observamos nas matrizes acima que algumas variáveis têm correlação bem alta.

Trabalhar com dados altamente correlacionados: 

 - Não acrescenta informação da variabilidade dos dados
 - Os modelos podem levar mais tempo que o necessário para serem ajustados
 
 Para ajudar nesse problema, vamos recorrer à técnicas de redução de dimensionalidade.
 
 
## PCA: Reduzindo a dimensão

Análise de componentes principais é uma técnica de fatorização de matriz em que é possível explicar (parte da) a variabilidade dos dados por meio de combinações lineares não correlacionadas dos dados originais. De maneira geral, a ideia de fatorar uma matriz é escrever uma matriz "complicada" no produto de duas matrizes "mais simples".

Vamos definir as componentes principais por meio de operações matriciais, para conhecer todo o processo. 

1. Estimando a matriz de correlação:

O primeiro passo é estimar a matriz de correlação dos nossos dados. Na verdade, nessa etapa usa-se a matriz de covariâncias, mas um problema que pode surgir é que algumas variância são maiores que outras, e em casos muito discrepantes isso distorce as componentes principais. Para evitar isso, o procedimento é: padronizar a matriz de covariâncias e estimar as componentes a partir dessa matriz padronizada. Mas isso é o mesmo que usar a matriz de correlação e por isso vou seguir por ela.

```{r cor-matrix}

upper <- cor_matrix
upper[upper.tri(cor_matrix)] <- NA 

knitr::kable(upper[1:5, 1:5],  caption = "Matriz de correlação: 5 primeiras linhas/colunas")
```

2. Obtendo os autovalores: 

```{r eigenvalues, collapse=TRUE}
eig$values

plot(eig$values)
abline(h = 1)
```

Para estimar o número de componentes principais ideal, um critério prático é observar os autovalores maiores ou iguais a 1, pois dessa forma as combinações lineares explicam a variância de uma variável original (padronizada) do dataset. Observando o scree-plot acima, é **k = 6** é o número de componentes principais adequados para explicar a estrutura da variância dos dados.

3. Obtendo os autovetores e escrevendo a 1ª componente principal:


Observando os autovetores associados aos seis autovalores identificados (apenas as 6 primeiras linhas de 30)

```{r eigenvectors, echo=FALSE, warning=FALSE, message=FALSE}

head(as.data.frame(eig$vectors[,1:6]))


```


Olhando um pouco para a teoria, a j-ésima componente principal é dada por:

$$
Ŷ_j = ê_{j1}X_{1} + ê_{j2}X_{2} + ... + ê_{jp}X_{p}
$$

E podemos escrever, para os dados do dataset em questão, a primeira componente principal da seguinte forma:

$$
Ŷ_1 = -0.2189\text{radius_mean} -0.1037\text{texture_mean} - 0.227\text{perimeter_mean} + ... - 0.132\text{fractal_dimension_worst}
$$

E assim em diante...

Mas como visualizações são mais interessantes, vamos construir gráficos!

#### Visualizando as componentes principais

Bom, como a função *fviz_pca_biplot* do pacote *factoextra* faz um gráfico apropriado a partir de um objeto da classe PCA, vou utilizar a função *princomp* (do stats, que faz parte do R base) para criar esse objeto. Essa função foi escolhida porque ela usa o **teorema da decomposição espectral**, que é a abordagem teórica utilizada na descrição acima. Caso a abordagem fosse **decomposição em valores singulares** (SVD), a função apropriada seria a *prcomp* (também do *stats*).

```{r pca-object}

pca <- princomp(data_processed[,-1], cor = TRUE, scores = TRUE)

fviz_pca_biplot(pca, col.ind = data_processed$diagnosis, col="black",
                palette = "Dark2", geom = "point", repel=TRUE,
                legend.title="Diagnóstico", addEllipses = TRUE) +
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) 
```

A primeira componente explica 44.3% da variância dos dados. É possível observar que existem variáveis correlacionadas negativamente, como `smoothness_se` e `radius_mean`, justamente os vetores mais distantes entre si. Também vemos que há ali um agrupamento das classes Benigno/Maligno.

Como o PCA foi utilizado para reduzir a dimensionalidade dos dados, os scores das 6 primeiras dimensões serão utilizados para ajustar o modelo de classificação.

## Classificando

Organizando os dados que serão utilizados no modelo:

```{r score}
data_scores <- 
  bind_cols(data_processed$diagnosis, as.data.frame(pca$scores[,1:6])) %>% 
  clean_names() %>% 
  rename(diagnosis = x1)
```

Dividindo os dados em conjuntos de treino e teste (70/30):

```{r train-test, message=FALSE}
train <- data_scores %>% sample_frac(.7)
test <- data_scores %>% anti_join(train)
```

### Modelo 01: Regressão logística

#### Ajustando o modelo

```{r logistic-1}
logistic_model1 <- glm(diagnosis ~ ., family = binomial, data = train)
summary(logistic_model1)
```

A variável `comp_6` parece **não** contribuir significativamente para classificarmos o diagnóstico (utilizando $\alpha = 0.05$). Um novo modelo pode ser ajustado removendo essa variável, já que variáveis irrelevantes podem deteriorar a taxa de erro do modelo.

```{r logistic-2}
logistic_model2 <- glm(diagnosis ~ comp_1 + comp_2 + comp_3 + comp_4 + comp_5, 
                       family = binomial, data = train)

summary(logistic_model2)
```

#### E como fica no conjunto de teste?

Queremos prever _novas_ observações, então vamos lá:

```{r predict}
pred <-
  predict(logistic_model2, test, type = "response") %>% 
  as.data.frame() %>% 
  rename(., "prob" = ".") %>% 
  bind_cols(test) %>% 
  mutate(predicted = as.factor(ifelse(prob > 0.5, "Malignant", "Benign")))

```

As probabilidades são dadas na forma $P(Y = 1 | X)$, e olhando para a dummy definida como 1

```{r}
contrasts(data_scores$diagnosis)
```

olhamos para os valores como sendo a probabilidade do tumor ser maligno.

#### Avaliando a acurácia do modelo

```{r conf-matrix, collapse=TRUE}

metrics_lg <- confusionMatrix(pred$predicted, pred$diagnosis)

cmat <- conf_mat(metrics_lg$table)
autoplot(cmat, type = "heatmap")

metrics_lg
```

A acurácia do modelo é de `r metrics_lg$overall[1]` e o coeficiente de Kappa `r metrics_lg$overall[2]`, que nos indica uma concordância entre o predito e os valores verdadeiros. É importante lembrar que a acurácia é uma métrica que deve ser usada com cuidado, pois ela pode não representar bem a situação. 

Na área da saúde, as medidas mais utilizadas são a especificidade e a sensibilidade, comuns em testes diagnósticos. Em termos práticos, a sensibilidade avalia a capacidade do classificador detectar o tumor maligno quando ele está presente. Já a especificidade nos dá a probabilidade de um teste dar negativo (no caso, benigno) quando, de fato, não há doença. A sensibilidade do classificador é de `r metrics_lg$byClass[1]`e a especifidade `r metrics_lg$byClass[2]`.

Quando estamos em um cenário de classificação, uma forma de checarmos a taxa de erro associada ao conjunto de teste é estimada da seguinte forma:


$Media(I(y_{0} \neq \hat{y_0}))$


Ou seja, é a média de uma variável indicadora que assume 1 quando a classe predita é igual a classe verdadeira e 0, caso contrário. Queremos o menor valor possível para essa média. Assim, considerando o problema, temos:

```{r}
mean(pred$diagnosis != pred$predicted)
```

Ou seja, o modelo de regressão logística previu incorretamente, em média, apenas `r round((mean(pred$diagnosis != pred$predicted)*100), 3)`% das vezes.

ROC e AUC:

```{r roc-auc, collapse=TRUE}
predr <- prediction(pred$prob, pred$diagnosis)
perf <- performance(predr, "tpr", "fpr")
plot(perf)


auc <- performance(predr, measure = 'auc') 
print(paste('AUC on Test', unlist(slot(auc, 'y.values'))))
```

### Modelo 02 - Árvores de decisão: Bagging

A ideia desta abordagem é fazer um bootstrap do conjunto de treino, ajustar árvores individuais para cada um dos datasets obtidos e depois juntá-los. Aqui estamos usando todos os preditores (6 variáveis). Essa técnica surgiu para redução de variância.


#### Ajustando o modelo
```{r bag-1}
bag <- randomForest(diagnosis ~ ., data = train, mtry = ncol(train) - 1, importance=TRUE)
bag
```

#### E como fica para novas observações?

```{r bag-2}
pred_bag <-
  predict(bag, newdata = test[,-1]) %>%
  as.data.frame() %>% 
  rename(., "pred" = ".") %>% 
  bind_cols(test)
```

#### Avaliando a acurácia do modelo

```{r bag-conf-matrix}
cmat_bag <- confusionMatrix(pred_bag$pred, pred_bag$diagnosis)
conf_bag <- conf_mat(cmat_bag$table)

autoplot(conf_bag, type = "heatmap")

cmat_bag
```

```{r roc-auc-bag, collapse=TRUE}
predprob <- predict(bag, newdata = test[,-1], type = "prob")[,2]

predr_bag <- prediction(predprob, pred_bag$diagnosis)
perf_bag <- performance(predr_bag, "tpr", "fpr")
plot(perf_bag)


auc_bag <- performance(predr_bag, measure = 'auc') 
print(paste('AUC on Test', unlist(slot(auc_bag, 'y.values'))))
```



## Modelo 03 - Árvores de decisão: Random Forests

A ideia desta abordagem é também fazer um bootstrap do conjunto de treino, ajustar árvores individuais para cada um dos datasets obtidos e depois juntá-los. A diferença aqui é que não são usados todos os preditores, apenas um subconjunto. Isso permite que as árvores ajustadas sejam bem diferentes entre si, pois a variável mais importante não é sempre usada para criar a separação da árvore.

Em geral, usa-se o número de preditores $m = sqrt(p)$, sendo p o número de preditores originais.


#### Ajustando o modelo
```{r rf-1}
rf <- randomForest(diagnosis ~ ., data = train, mtry = sqrt(ncol(train) - 1), importance=TRUE)
rf
```

#### E como fica para novas observações?

```{r rf-2}
pred_rf <-
  predict(rf, newdata = test[,-1]) %>%
  as.data.frame() %>% 
  rename(., "pred" = ".") %>% 
  bind_cols(test)
```


#### Avaliando a acurácia do modelo

```{r rf-conf-matrix}
cmat_rf <- confusionMatrix(pred_rf$pred, pred_rf$diagnosis)
conf_rf <- conf_mat(cmat_rf$table)

autoplot(conf_rf, type = "heatmap")

cmat_rf
```

```{r roc-auc-rf}
predprob_rf <- predict(rf, newdata = test[,-1], type = "prob")[,2]

predr_rf <- prediction(predprob_rf, pred_rf$diagnosis)
perf_rf <- performance(predr_rf, "tpr", "fpr")
plot(perf_rf)


auc_rf <- performance(predr_rf, measure = 'auc') 
print(paste('AUC on Test', unlist(slot(auc_rf, 'y.values'))))
```

## Concluindo

```{r}
print(paste('Logistic Regression: AUC on Test', unlist(slot(auc, 'y.values'))))
print(paste('Bagging: AUC on Test', unlist(slot(auc_bag, 'y.values'))))
print(paste('Random Forests: AUC on Test', unlist(slot(auc_rf, 'y.values'))))
```

O modelo que apresentou a maior área abaixo da curva foi o mais simples, a **regressão logística**. É o modelo mais restrito apresentado, mas também é o que permite uma interpretabilidade maior que os mais sofisticados. Também vale notar que o **random forest** apresentou um desempenho bem próximo.

Pretendo estudar formas de contornar as classes desbalanceadas (62.7/37.3) desse problema que, apesar de existirem problemas com classes mais díspares, a diferença aqui pode sim desviar a classificação para a classe majoritária.

## Referências

- https://www.inca.gov.br/publicacoes/livros/situacao-do-cancer-de-mama-no-brasil-sintese-de-dados-dos-sistemas-de-informacao

- https://www.kaggle.com/gpreda/breast-cancer-prediction-from-cytopathology-data/report

- https://www.kaggle.com/mirichoi0218/classification-breast-cancer-or-not-with-15-ml/comments

- _An Introduction to Statistical Learning: With Applications in R_ por Robert Tibshirani e Trevor Hastie

- _Análise de dados através de métodos de estatística multivariada_ por Sueli Aparecida Mingoti